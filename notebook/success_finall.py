# -*- coding: utf-8 -*-
"""success_finall.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ya8-OECN-mzQgfAudJLVzlwQ4W9eN78H

# **Start-Up Success Prediction**

### Introduction

This model aims to predict the success of a startup by leveraging historical data that includes funding characteristics, milestones, and relational networks. Using a binary classification approach based on deep learning, the model is designed to assist investors and financial institutions in identifying high-potential startups that are likely to succeed and are suitable for funding.

In this project, we develop two predictive models using TensorFlow, with a focus on:

1. Assessing the potential success of a startup.
2. Providing an objective evaluation basis for investors and financial institutions.

### Problem Statement

Many investors and financial institutions, such as banks, face significant challenges in determining which startups are worthy of investment. One of the main obstacles is the limited access to accurate, transparent, and standardized financial data from startups. The funding eligibility assessment process is often time-consuming, subjective, and heavily reliant on the evaluatorâ€™s personal experience.

As a result, promising investment opportunities may be overlooked or rejected due to suboptimal assessment. By applying machine learning techniques, this evaluation process can be accelerated, standardized, and supported by insights derived from relevant historical data.

The dataset used in this study is obtained from Kaggle:
ðŸ”— [Startup Success Prediction Dataset](https://www.kaggle.com/datasets/manishkc06/startup-success-prediction?select=startup+data.csv)

#Import Library
"""

# Data & Visualisasi
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from scipy.stats.mstats import winsorize
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.class_weight import compute_class_weight

# Evaluasi
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
    ConfusionMatrixDisplay,
    auc as sklearn_auc,
    auc
)

# TensorFlow / Keras
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.losses import BinaryFocalCrossentropy
from tensorflow.keras import regularizers

# Utility
import joblib
from google.colab import drive

"""# Gathering Data"""

drive.mount('/content/drive')
df = pd.read_csv('/content/drive/MyDrive/datacp_dbs/startup.csv')

"""# EDA"""

df.head()

df.info()

df.isna().sum()

df.describe()

df.duplicated().sum()

plt.figure(figsize=(12,10))
sns.heatmap(df.select_dtypes(include=[np.number]).corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

correlation = df.select_dtypes(include=[np.number]).corr()
target_correlation = correlation['labels']
threshold = 0.1
significant_features = target_correlation[(target_correlation > threshold) | (target_correlation < -threshold)]
print("Significant Features:")
print(significant_features)

plt.figure(figsize=(10, 8))
sns.heatmap(df[significant_features.index].corr(), annot=True, cmap='coolwarm')
plt.title("Heatmap dari Fitur yang Berkorelasi Signifikan dengan Target")
plt.show()

sns.countplot(x='labels', data=df)
plt.title("Distribusi Target (Labels)")
plt.show()

numerical_features = df.select_dtypes(include=[np.number]).columns.drop('labels')
rows = int(np.ceil(len(numerical_features) / 4))
fig, axes = plt.subplots(nrows=rows, ncols=4, figsize=(25, rows * 3))
axes = axes.flatten()

for i, col in enumerate(numerical_features):
    sns.histplot(df[col], bins=30, kde=True, color='steelblue', ax=axes[i])
    axes[i].set_title(f'Distribusi: {col}')
    axes[i].set_xlabel('')
    axes[i].set_ylabel('Frekuensi')

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.suptitle("Distribusi Seluruh Fitur Numerik", fontsize=16, y=1.03)
plt.show()

fig, axes = plt.subplots(nrows=rows, ncols=4, figsize=(25, rows * 3))
axes = axes.flatten()

for i, col in enumerate(numerical_features):
    sns.boxplot(y=df[col], ax=axes[i], color='salmon')
    axes[i].set_title(f'Boxplot: {col}')

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.suptitle("Boxplot Seluruh Fitur Numerik", fontsize=16, y=1.03)
plt.show()

"""### **Exploratory Data Analysis (EDA) Insights**

#### Full Correlation Matrix

The first figure presents a comprehensive correlation matrix among all numerical and binary features. A red hue approaching 1 indicates a strong positive correlation, while a blue hue approaching -1 indicates a strong negative correlation. Features such as `relationships`, `milestones`, and `is_top500` exhibit notable correlations with the target variable `labels`.

#### Heatmap of Features Most Correlated with the Target

The second figure illustrates a heatmap highlighting features that are most significantly correlated with the target variable. Features such as `relationships` (0.36), `milestones` (0.33), and `is_top500` (0.31) emerge as strong candidates for key features due to their contribution to class distinction.

#### Target Distribution

The third figure shows the distribution of the target labels. It is evident that the proportion of successful startups (label = 1) is higher than that of unsuccessful ones (label = 0), indicating class imbalance. This issue will be addressed using the SMOTE technique during the preprocessing stage.

#### Distribution of Numerical Features

The fourth figure presents histograms of all numerical features. Many features exhibit a right-skewed distribution, such as `funding_total_usd` and `avg_participants`, indicating that most values lie in the lower range, with only a few instances having extremely high values.

#### Boxplot of Numerical Features

The fifth figure displays boxplots of the numerical features to identify outliers. Nearly all features contain outliers, particularly those related to funding and milestone age. These outliers were mitigated using the winsorizing technique to minimize their impact on model training.

Initial data exploration was conducted to understand the data structure, target distribution, and relationships between features. The dataset contains no duplicate entries but does include missing values in the `milestones` column, which were subsequently imputed. Outliers are prominent in several numerical features, especially in `funding_total_usd`, and were addressed through winsorization.

The target distribution reveals that the number of successful startups (label = 1) exceeds the number of failed ones (label = 0), indicating a class imbalance that will be handled during preprocessing.

From the numerical correlation analysis with the target (`labels`), the following features were identified as having the most significant correlations:

* `relationships` (0.36)
* `milestones` (0.33)
* `is_top500` (0.31)
* `age_last_milestone_year` (0.27)
* `has_roundB` (0.21)
* `funding_rounds` (0.21)

These features form the core basis for feature selection due to their strong relationships with startup success. A dedicated heatmap visualization was also employed to highlight features with significant correlations to the target. Most numerical features display right-skewed distributions, indicating the dominance of lower values among the startup population.

# Data Preprosessing
"""

lst = ['age_first_milestone_year','age_last_milestone_year']
df[lst] = df[lst].fillna(0)

df.rename(columns={
    'is_top500': 'populer',
    'age_first_milestone_year': 'umur_milestone_pertama',
    'age_last_milestone_year': 'umur_milestone_terakhir',
    'relationships': 'relasi',
    'funding_rounds': 'jumlah_pendanaan',
    'milestones': 'jumlah_milestone',
    'avg_participants': 'rata_partisipan',
    'age_first_funding_year': 'umur_pendanaan_pertama',
    'age_last_funding_year': 'umur_pendanaan_terakhir',
    'funding_total_usd': 'total_dana',
    'category_code' : 'kategori'
}, inplace=True)

df = df.drop(columns=['Unnamed: 0', 'id', 'object_id', 'name', 'city', 'zip_code',
                      'latitude', 'longitude', 'Unnamed: 6', 'state_code', 'state_code.1',
                      'is_CA', 'is_TX', 'is_NY', 'is_otherstate', 'founded_at', 'closed_at',
                      'first_funding_at', 'last_funding_at', 'status', 'has_roundC', 'has_roundD',
                      'is_MA', 'is_software', 'is_web', 'is_mobile', 'is_enterprise',
                      'is_advertising', 'is_gamesvideo', 'is_ecommerce', 'is_biotech', 'is_consulting',
                      'is_othercategory', 'has_VC', 'has_angel','has_roundA','has_roundB'], errors='ignore')

df.head()

USD_TO_IDR = 16000
money_columns = ['total_dana', 'dana_per_pendanaan', 'rasio_dana_per_relasi']

for col in money_columns:
    if col in df.columns:
        df[col] = df[col] * USD_TO_IDR

df.info()

for col in ['umur_milestone_pertama', 'umur_milestone_terakhir', 'relasi', 'jumlah_pendanaan', 'jumlah_milestone', 'rata_partisipan']:
    df[col] = winsorize(df[col], limits=[0.01, 0.01])

categorical_cols = ['kategori']
numerical_cols = [
    'umur_milestone_terakhir', 'relasi', 'umur_pendanaan_pertama',
    'total_dana', 'umur_pendanaan_terakhir', 'umur_milestone_pertama',
    'rata_partisipan', 'kategori', 'jumlah_pendanaan', 'jumlah_milestone'
]

df['rasio_dana_per_relasi'] = df['total_dana'] / (df['relasi'] + 1)
df['dana_per_pendanaan'] = df['total_dana'] / (df['jumlah_pendanaan'] + 1)

numerical_cols.extend(['rasio_dana_per_relasi', 'dana_per_pendanaan'])

le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

rows = int(np.ceil(len(numerical_cols) / 4))
fig, axes = plt.subplots(nrows=rows, ncols=4, figsize=(25, rows * 3))
axes = axes.flatten()

for i, col in enumerate(numerical_cols):
    sns.histplot(df[col], bins=30, kde=True, color='skyblue', ax=axes[i])
    axes[i].set_title(f"Distribusi: {col}")
    axes[i].set_xlabel("")
    axes[i].set_ylabel("Frekuensi")

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.suptitle("Distribusi Fitur Numerik Setelah Preprocessing", fontsize=16, y=1.03)
plt.show()

joblib.dump(scaler, 'scaler_startup_success.pkl')

joblib.dump(le, 'label_encoder_kategori.pkl')

df.to_csv("startup_data_preprocessed.csv", index=False)

"""### **Data Preprocessing Insights**

Following initial exploration and cleaning, a number of key features were selected based on their correlation with the target variable `labels` as well as their importance scores derived from a Random Forest model. The features used for model training include:

* `age_last_milestone_year`
* `relationships`
* `age_first_funding_year`
* `funding_total`
* `age_last_funding_year`
* `age_first_milestone_year`
* `avg_participants`
* `category` (encoded)
* `funding_rounds`
* `milestones`
* `funding_per_relationship`
* `funding_per_round`
* `popularity`

The preprocessing stage aims to prepare the data for model training. This process includes the following key steps:

1. **Handling Missing Values:**
   Missing values in features such as `age_first_milestone_year` and `age_last_milestone_year` were imputed with zeros (a conservative strategy) to preserve the dataset size without introducing bias.

2. **Renaming Columns:**
   Several columns were renamed for clarity and consistency. For example, `is_top500` was renamed to `popularity`, `relationships` to `relationships`, and `funding_total_usd` to `funding_total`.

3. **Removing Irrelevant Columns:**
   Columns such as ID, geographic location, and specific funding statuses were removed due to their limited predictive value or redundancy with other features. In addition, the `funding_total` values were converted from USD to Indonesian Rupiah for localization.

4. **Outlier Handling:**
   Numerical features containing extreme values, such as `relationships`, `funding_rounds`, and `milestones`, were treated using the **winsorizing** technique, with thresholds set at the 1st and 99th percentiles.

5. **Encoding Categorical Features:**
   The string-based `category` feature was transformed into numeric form using `LabelEncoder`, which was then saved for future inference purposes.

6. **Feature Engineering:**
   Two new features were engineered to enhance data representation:

   * `funding_per_relationship` = `funding_total` / (`relationships` + 1)
   * `funding_per_round` = `funding_total` / (`funding_rounds` + 1)

7. **Scaling Numerical Features:**
   All numerical features, including the engineered ones (`funding_per_relationship` and `funding_per_round`), were scaled using `StandardScaler` to ensure a standard distribution (mean = 0, standard deviation = 1).

Upon completion of the preprocessing pipeline, the resulting dataset was saved as `startup_data_preprocessed.csv`. Additionally, the `LabelEncoder` and `StandardScaler` objects were preserved using `joblib` to ensure consistency during future inference.

# Modeling & Evaluasi
"""

X = df.drop(columns='labels')
y = df['labels']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print("Distribusi y_train:", np.bincount(y_train))
print("Distribusi y_test:", np.bincount(y_test))

rf = RandomForestClassifier().fit(X_train, y_train)
importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)
print("Feature Importances (Random Forest):", importances)

input_dim = X_train.shape[1]
model = Sequential([
    Input(shape=(input_dim,)),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss='binary_crossentropy',
    metrics=[tf.keras.metrics.AUC(name='auc')]
)

class_weights_array = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights_array))
print("Class Weights:", class_weights)

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_auc', mode='max', factor=0.5, patience=10, verbose=1)

model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=32,
    callbacks=[early_stop, reduce_lr],
    class_weight=class_weights,
    verbose=1
)

y_pred_proba = model.predict(X_test)
y_pred = (y_pred_proba > 0.5).astype(int)

test_auc = roc_auc_score(y_test, y_pred_proba)
print(f"Test AUC: {test_auc:.4f}")

probs = model.predict(X_test)
fpr, tpr, thresholds = roc_curve(y_test, probs)
roc_auc = sklearn_auc(fpr, tpr)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
pr_auc = auc(recall, precision)

plt.figure(figsize=(6, 6))
plt.plot(recall, precision, label=f'PR Curve (area = {pr_auc:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')
plt.grid(True)
plt.show()

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

"""### **Modelling and Evaluation Insights**

Prior to training, the dataset was split into training and test sets in an 80:20 ratio using `train_test_split`, with stratification based on the target labels to preserve class distribution. Class imbalance was addressed using a **class weighting** strategy, which offers more stability compared to oversampling methods such as SMOTE, as it avoids data duplication and reduces the risk of overfitting.

To understand each featureâ€™s contribution to the target variable, a Random Forest Classifier was used to calculate feature importance. The most significant features included `age_last_milestone_year`, `funding_total`, and `funding_per_round`, which were then used as inputs for developing an Artificial Neural Network (ANN) model.

The model was implemented using the Keras Sequential API with three hidden layers containing 128, 64, and 32 units, respectively, each utilizing the ReLU activation function. Dropout layers were added after the first and second hidden layers to mitigate overfitting. The output layer used a sigmoid activation function, appropriate for binary classification tasks. The model was compiled with the `binary_crossentropy` loss function and the Adam optimizer, with a learning rate of 1e-3. The primary evaluation metric was AUC (Area Under the Curve).

To ensure training stability and prevent overfitting, **EarlyStopping** was applied with a `patience` of 10 and `restore_best_weights=True`. In addition, **ReduceLROnPlateau** was employed to dynamically decrease the learning rate when the validation metric plateaued.

---

### **Model Evaluation**

Upon completion of training, the model's performance was evaluated on the test dataset. The evaluation results indicate that the model performs well in distinguishing between the target classes.

#### Evaluation Results

* **Train Accuracy**: 0.8686
* **Best Validation AUC**: 0.8258
* **Test AUC**: 0.8286
* **ROC AUC Score**: 0.83
* **Precision-Recall AUC**: 0.89

#### Evaluation Graphs

1. **Accuracy and Loss Curves**

   * The accuracy graph demonstrates a consistent improvement on both training and validation sets, without significant signs of overfitting.
   * The loss curve shows a steady decline, indicating convergence during the training process.

2. **ROC Curve**

   * An AUC ROC score of 0.83 reflects the modelâ€™s strong capability in distinguishing between positive and negative classes.

3. **Precision-Recall Curve**

   * An AUC of 0.89 highlights excellent model performance, particularly in the context of class imbalance, where precision and recall metrics are more informative.

4. **Confusion Matrix**

   * **True Positives**: 93
   * **True Negatives**: 46
   * **False Positives**: 19
   * **False Negatives**: 27

   These results suggest that the model is relatively more accurate in identifying the positive class (label = 1), aligning well with the primary objective of detecting potentially successful startups.

# Saved Model
"""

model.save('best_model.h5')

model.save('model_startup.keras')

!pip install tensorflowjs
!tensorflowjs_converter --input_format=keras best_model.h5 tfjs_model

from google.colab import files
!zip -r tfjs_model.zip tfjs_model
files.download('tfjs_model.zip')

"""# Predict"""

le = joblib.load('label_encoder_kategori.pkl')
label_mapping = {label: idx for idx, label in enumerate(le.classes_)}
print("Mapping LabelEncoder untuk 'kategori':")
for k, v in label_mapping.items():
    print(f"{k} â†’ {v}")

# 1. Sample input
sample_input = pd.DataFrame([{
    'umur_milestone_terakhir': 2.5,
    'relasi': 8,
    'umur_pendanaan_pertama': 1.2,
    'total_dana': 12000000,
    'umur_pendanaan_terakhir': 2.0,
    'umur_milestone_pertama': 1.0,
    'rata_partisipan': 3,
    'kategori': 'music',
    'jumlah_pendanaan': 4,
    'jumlah_milestone': 4,
    'rasio_dana_per_relasi': 150000.0,
    'dana_per_pendanaan': 300000.0,
    'populer': 1
}])

# 2. Encode kategori
le = joblib.load('label_encoder_kategori.pkl')
sample_input['kategori'] = le.transform(sample_input['kategori'])

# 3. Scaling fitur numerik
scaler = joblib.load('scaler_startup_success.pkl')
scaled_features = scaler.transform(sample_input[scaler.feature_names_in_])
sample_scaled = np.concatenate([scaled_features, sample_input[['populer']].values], axis=1)

# 4. Load model dan prediksi
model = load_model('best_model.h5')
prediction = model.predict(sample_scaled)

# 5. Konversi ke label
binary_result = int(prediction[0][0] > 0.5)
label = "Sukses" if binary_result == 1 else "Gagal"

print(f"Hasil prediksi: {label} (Probabilitas: {prediction[0][0]:.4f})")

# 1. Sample input
sample_input = pd.DataFrame([{
    'umur_milestone_terakhir': 0.5,
    'relasi': 0,
    'umur_pendanaan_pertama': 0.2,
    'total_dana': 50000000,
    'umur_pendanaan_terakhir': 0.3,
    'umur_milestone_pertama': 0.4,
    'rata_partisipan': 0,
    'kategori': 'games_video',
    'jumlah_pendanaan': 0,
    'jumlah_milestone': 0,
    'rasio_dana_per_relasi': 5000.0,
    'dana_per_pendanaan': 5000.0,
    'populer': 0
}])


# 2. Encode kategori
le = joblib.load('label_encoder_kategori.pkl')
sample_input['kategori'] = le.transform(sample_input['kategori'])

# 3. Scaling
scaler = joblib.load('scaler_startup_success.pkl')
scaled_features = scaler.transform(sample_input[scaler.feature_names_in_])
sample_scaled = np.concatenate([scaled_features, sample_input[['populer']].values], axis=1)

# 4. Load model dan prediksi
model = load_model('best_model.h5')
prediction = model.predict(sample_scaled)

# 5. Konversi ke label
binary_result = int(prediction[0][0] > 0.5)
label = "Sukses" if binary_result == 1 else "Gagal"

print(f"Hasil prediksi: {label}")

"""### **Inference**

To perform predictions on new startup data, an inference pipeline was implemented following these steps:

1. The input data is provided in the form of a `DataFrame`, aligned with the exact feature order used during model training.
2. The `category` feature is encoded using the previously saved `LabelEncoder` object.
3. Numerical features are scaled using the same `StandardScaler` applied during training.
4. The `popularity` feature is manually re-integrated, as it was excluded from the scaling process.
5. The trained model (`best_model.h5`) is loaded and used to generate probabilistic predictions.
6. The probabilistic outputs are then converted to binary labels based on the following threshold:

   * â‰¥ 0.5 â†’ **Successful (Sukses)**
   * < 0.5 â†’ **Unsuccessful (Gagal)**

#### Example Predictions:

* **Sample 1** â†’ Successful/Sukses
* **Sample 2** â†’ Unsuccessful/Gagal

In the current test scenario, all samples were predicted as **Successful**, which may indicate the model's tendency toward the majority class. Further analysis and threshold calibration may be necessary to improve predictive balance, especially in the presence of class imbalance.

"""

!pip list